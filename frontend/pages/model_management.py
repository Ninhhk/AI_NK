import streamlit as st
import requests
import json
import time
import os
from io import BytesIO
import sys
from pathlib import Path

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))
from frontend.components.system_prompt import system_prompt_ui

# Set page config for better appearance
st.set_page_config(
    page_title="Qu·∫£n L√Ω Model AI",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Load custom CSS
with open('frontend/style.css') as f:
    st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)       

# Add custom styles for model management
st.markdown("""
    <style>
    .model-card {
        background-color: var(--card-background);
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
        border: 1px solid rgba(255, 255, 255, 0.1);
        transition: all 0.3s ease;
    }
    .model-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(52, 152, 219, 0.2);
    }
    .model-progress {
        background-color: var(--primary-color-light);
        border-radius: 5px;
        height: 12px;
        margin-top: 0.5rem;
        overflow: hidden;
    }
    .model-progress-inner {
        background: linear-gradient(to right, var(--primary-color), var(--accent-color));
        height: 100%;
        transition: width 0.3s ease;
    }
    .model-actions {
        display: flex;
        gap: 0.5rem;
        margin-top: 1rem;
    }
    .model-icon {
        font-size: 2rem;
        margin-right: 1rem;
    }
    .model-header {
        display: flex;
        align-items: center;
    }
    </style>
""", unsafe_allow_html=True)

# API endpoints
OLLAMA_API_URL = "http://localhost:8000/api/ollama"
SLIDES_API_URL = "http://localhost:8000/api/slides"

def get_models():
    """L·∫•y t·∫•t c·∫£ models c√≥ s·∫µn t·ª´ API"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/models")
        response.raise_for_status()
        return response.json()["models"]
    except Exception as e:
        st.error(f"L·ªói khi l·∫•y models: {e}")
        return []

def get_current_model():
    """L·∫•y model ƒëang ho·∫°t ƒë·ªông hi·ªán t·∫°i cho vi·ªác t·∫°o slide"""
    try:
        response = requests.get(f"{SLIDES_API_URL}/current-model")
        response.raise_for_status()
        return response.json()["model_name"]
    except Exception as e:
        st.error(f"L·ªói khi l·∫•y model hi·ªán t·∫°i: {e}")
        return None

def set_model(model_name):
    """ƒê·∫∑t model cho vi·ªác t·∫°o slide"""
    try:
        response = requests.post(
            f"{SLIDES_API_URL}/set-model",
            data={"model_name": model_name}
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"L·ªói khi ƒë·∫∑t model: {e}")
        return None

def pull_model(model_name):
    """B·∫Øt ƒë·∫ßu t·∫£i model t·ª´ Ollama"""
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/models/pull",
            data={"model_name": model_name}
        )
        response.raise_for_status()
        return model_name  # Tr·∫£ v·ªÅ t√™n model l√†m task ID
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i model: {e}")
        return None

def get_download_progress(task_id):
    """L·∫•y ti·∫øn tr√¨nh t·∫£i xu·ªëng cho m·ªôt task c·ª• th·ªÉ"""
    try:
        # Trong phi√™n b·∫£n ƒë∆°n gi·∫£n, ch√∫ng ta kh√¥ng theo d√µi ti·∫øn tr√¨nh
        # N√™n ch√∫ng ta s·∫Ω ch·ªâ tr·∫£ v·ªÅ m·ªôt object gi·∫£
        return {"done": True, "pull_progress": 1000, "error": None}
    except Exception as e:
        return {"done": False, "pull_progress": 0, "error": str(e)}

def get_all_progress():
    """L·∫•y t·∫•t c·∫£ ti·∫øn tr√¨nh t·∫£i xu·ªëng"""
    try:
        # Trong phi√™n b·∫£n ƒë∆°n gi·∫£n, ch√∫ng ta kh√¥ng theo d√µi ti·∫øn tr√¨nh
        return {}
    except Exception as e:
        return {}

def cancel_model_pull(task_id):
    """H·ªßy vi·ªác t·∫£i model"""
    # Trong phi√™n b·∫£n ƒë∆°n gi·∫£n, ch√∫ng ta kh√¥ng h·ªó tr·ª£ h·ªßy
    return True

def delete_model(model_name):
    """X√≥a m·ªôt model"""
    try:
        response = requests.delete(f"{OLLAMA_API_URL}/models/{model_name}") 
        response.raise_for_status()
        return response.json()["success"]
    except Exception as e:
        st.error(f"L·ªói khi x√≥a model: {e}")
        return False

def upload_model(file, model_name=None):
    """T·∫£i l√™n file model"""
    # Trong phi√™n b·∫£n ƒë∆°n gi·∫£n, ch√∫ng ta kh√¥ng h·ªó tr·ª£ t·∫£i l√™n file
    st.warning("T·∫£i l√™n file model kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ trong phi√™n b·∫£n n√†y")      
    return False

def get_system_prompt():
    """L·∫•y system prompt hi·ªán t·∫°i."""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/system-prompt")
        response.raise_for_status()
        return response.json()["system_prompt"]
    except Exception as e:
        st.error(f"L·ªói khi l·∫•y system prompt: {e}")
        return ""

def set_system_prompt(prompt):
    """ƒê·∫∑t system prompt."""
    try:
        response = requests.post(
            f"{OLLAMA_API_URL}/system-prompt",
            data={"system_prompt": prompt}
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"L·ªói khi ƒë·∫∑t system prompt: {e}")
        return None

# Header section with animated logo and title
col1, col2, col3 = st.columns([1,2,1])
with col2:
    st.markdown("""
        <div class="fade-in">
            <h1 style='text-align: center; color: var(--primary-color); font-size: 2.5em; margin-bottom: 0.5em;'>
                ü§ñ Qu·∫£n L√Ω Model
            </h1>
            <h3 style='text-align: center; color: var(--text-secondary); font-size: 1.2em;'>
                Qu·∫£n L√Ω C√°c Model AI Cho T·∫•t C·∫£ T√≠nh NƒÉng ·ª®ng D·ª•ng
            </h3>
        </div>
    """, unsafe_allow_html=True)

# Main content area with animated separator
st.markdown("""
    <div style='height: 2px; background: linear-gradient(90deg, transparent, var(--primary-color), transparent);'></div>
""", unsafe_allow_html=True)

# Tabs for different sections
tab1, tab2, tab3, tab4 = st.tabs(["Models C√≥ S·∫µn", "Th√™m Model M·ªõi", "System Prompt", "ƒêang T·∫£i Xu·ªëng"])

with tab1:
    # L·∫•y model hi·ªán t·∫°i
    current_model = get_current_model()
      # N√∫t l√†m m·ªõi
    if st.button("üîÑ L√†m M·ªõi Models", type="primary"):
        st.rerun()
    
    # L·∫•y models
    models = get_models()
    
    if not models:
        st.info("Kh√¥ng c√≥ models n√†o c√≥ s·∫µn. Th√™m models trong tab 'Th√™m Model M·ªõi'.")
    else:
        st.markdown(f"### Models C√≥ S·∫µn ({len(models)})")
        st.markdown("Ch·ªçn m·ªôt model ƒë·ªÉ s·ª≠ d·ª•ng cho t·∫•t c·∫£ t√≠nh nƒÉng (t·∫°o slide, ph√¢n t√≠ch t√†i li·ªáu v√† t·∫°o quiz)")
        
        # Hi·ªÉn th·ªã th√¥ng b√°o to√†n c·ª•c v·ªÅ model hi·ªán t·∫°i
        if current_model:
            st.info(f"üîÑ **Model ƒêang Ho·∫°t ƒë·ªông: {current_model}** - Model n√†y hi·ªán ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng cho t·∫•t c·∫£ t√≠nh nƒÉng ·ª©ng d·ª•ng.", icon="‚ÑπÔ∏è")
        
        for model in models:
            with st.container():
                st.markdown(f"""
                <div class="model-card">
                    <div class="model-header">
                        <div class="model-icon">üß†</div>
                        <div>
                            <h3 style="margin: 0;">{model['name']}</h3>     
                            <p style="margin: 0; color: var(--text-secondary);">
                                K√≠ch th∆∞·ªõc: {model['size'] / (1024*1024):.1f} MB | S·ª≠a ƒë·ªïi: {model['modified_at']}
                            </p>
                        </div>
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # C√°c n√∫t h√†nh ƒë·ªông trong c·ªôt
                col1, col2, col3, col4 = st.columns([1, 1, 1, 3])
                with col1:
                    if st.button(f"Ch·ªçn", key=f"select_{model['name']}"):
                        result = set_model(model['name'])
                        if result:
                            st.success(f"Model ƒë√£ ƒë∆∞·ª£c thay ƒë·ªïi th√†nh {model['name']} v√† s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng cho t·∫•t c·∫£ t√≠nh nƒÉng")
                            time.sleep(1)
                            st.rerun()
                
                with col2:
                    if st.button(f"Th√¥ng tin", key=f"info_{model['name']}"):
                        st.json(model['details'] if 'details' in model else model)
                with col3:
                    if st.button(f"X√≥a", key=f"delete_{model['name']}"):
                        if model['name'] == current_model:
                            st.error("Kh√¥ng th·ªÉ x√≥a model ƒëang ho·∫°t ƒë·ªông")
                        else:
                            if delete_model(model['name']):
                                st.success(f"Model {model['name']} ƒë√£ ƒë∆∞·ª£c x√≥a")
                                time.sleep(1)
                                st.rerun()
                
                with col4:
                    if model['name'] == current_model:
                        st.success("‚úÖ Model to√†n c·ª•c hi·ªán t·∫°i")

with tab2:
    st.markdown("### Th√™m Model M·ªõi")
    
    # Ph·∫ßn t·∫£i model
    with st.expander("T·∫£i Model t·ª´ Ollama", expanded=True):
        st.markdown("""
        Nh·∫≠p t√™n c·ªßa m·ªôt model Ollama ƒë·ªÉ t·∫£i xu·ªëng. V√≠ d·ª•:
        - `llama3:8b`
        - `gemma3:1b`
        - `mistral:latest`
        """)
        
        with st.form("pull_model_form"):
            model_name = st.text_input("T√™n Model", placeholder="v√≠ d·ª•: llama3:8b")
            submitted = st.form_submit_button("B·∫Øt ƒê·∫ßu T·∫£i Xu·ªëng")
            if submitted and model_name:
                task_id = pull_model(model_name)
                if task_id:
                    st.success(f"ƒê√£ b·∫Øt ƒë·∫ßu t·∫£i xu·ªëng {model_name}")
                    time.sleep(1)
                    st.rerun()
    
    # Ph·∫ßn t·∫£i l√™n model
    with st.expander("T·∫£i L√™n File Model GGUF", expanded=False):
        st.markdown("""
        T·∫£i l√™n tr·ª±c ti·∫øp file model GGUF. File n√†y s·∫Ω ƒë∆∞·ª£c th√™m v√†o Ollama.
        """)
        
        with st.form("upload_model_form"):
            uploaded_file = st.file_uploader("Ch·ªçn file GGUF", type=["gguf"])
            custom_name = st.text_input("T√™n Model T√πy Ch·ªânh (t√πy ch·ªçn)")
            upload_submitted = st.form_submit_button("T·∫£i L√™n Model")
            
            if upload_submitted and uploaded_file:
                success = upload_model(uploaded_file, custom_name if custom_name else None)
                if success:
                    st.success(f"Model ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n th√†nh c√¥ng")
                    time.sleep(1)
                    st.experimental_rerun()

with tab3:
    st.markdown("### System Prompt To√†n C·ª•c")
    st.markdown("""
    <div class="card fade-in">
        <p>
            System prompt ƒë∆∞·ª£c s·ª≠ d·ª•ng cho t·∫•t c·∫£ t√≠nh nƒÉng c·ªßa ·ª©ng d·ª•ng ƒë·ªÉ ki·ªÉm so√°t c√°ch AI ph·∫£n h·ªìi. 
            ƒê√¢y l√† c√†i ƒë·∫∑t to√†n c·ª•c ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác t·∫°o slide, ph√¢n t√≠ch t√†i li·ªáu v√† t·∫•t c·∫£ t∆∞∆°ng t√°c AI kh√°c.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Tr∆∞·ªõc ti√™n l·∫•y system prompt hi·ªán t·∫°i t·ª´ API
    current_system_prompt = ""
    try:
        current_system_prompt = get_system_prompt()
    except:
        # N·∫øu API kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng prompt m·∫∑c ƒë·ªãnh tr·ªëng
        pass
        
    # Hi·ªÉn th·ªã component system prompt UI
    system_prompt = system_prompt_ui(default_prompt=current_system_prompt, key_prefix="model_management")
    
    # Th√™m n√∫t ƒë·ªÉ l∆∞u system prompt to√†n c·ª•c
    col1, col2 = st.columns([1, 3])
    with col1:
        if st.button("üíæ L∆∞u System Prompt To√†n C·ª•c", type="primary"):
            result = set_system_prompt(system_prompt)
            if result:
                st.success("‚úÖ System prompt ƒë√£ ƒë∆∞·ª£c l∆∞u to√†n c·ª•c v√† s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng cho t·∫•t c·∫£ t√≠nh nƒÉng")
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ l∆∞u system prompt")
    
    with col2:
        st.warning("ƒêi·ªÅu n√†y s·∫Ω c·∫≠p nh·∫≠t system prompt cho **t·∫•t c·∫£ t√≠nh nƒÉng** c·ªßa ·ª©ng d·ª•ng.")
    
    # Th√™m n√∫t ƒë·ªÉ ch·∫°y script set_system_prompt.py
    st.markdown("---")
    st.markdown("### ƒê·∫∑t L·∫°i Th√†nh Prompt Ti·∫øng Vi·ªát")
    
    if st.button("üîÑ ƒê·∫∑t L·∫°i Th√†nh Prompt Ti·∫øng Vi·ªát"):
        try:
            vietnamese_prompt = "\\no_think must answer in vietnamese, ph·∫£i tr·∫£ l·ªùi b·∫±ng ti·∫øng vi·ªát"
            result = set_system_prompt(vietnamese_prompt)
            if result:
                st.success("‚úÖ System prompt ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t l·∫°i th√†nh y√™u c·∫ßu ph·∫£n h·ªìi ti·∫øng Vi·ªát")
                time.sleep(1)
                st.experimental_rerun()
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ ƒë·∫∑t l·∫°i system prompt")
        except Exception as e:
            st.error(f"L·ªói khi ƒë·∫∑t l·∫°i system prompt: {e}")

with tab4:
    st.markdown("### ƒêang T·∫£i Xu·ªëng")
    
    # T·ª± ƒë·ªông l√†m m·ªõi cho ti·∫øn tr√¨nh t·∫£i xu·ªëng
    auto_refresh = st.checkbox("T·ª± ƒë·ªông l√†m m·ªõi (m·ªói 2 gi√¢y)", value=True)
    
    # L·∫•y t·∫•t c·∫£ ti·∫øn tr√¨nh t·∫£i xu·ªëng
    download_progress = get_all_progress()
    
    if not download_progress:
        st.info("Kh√¥ng c√≥ t·∫£i xu·ªëng n√†o ƒëang ho·∫°t ƒë·ªông. B·∫Øt ƒë·∫ßu t·∫£i xu·ªëng trong tab 'Th√™m Model M·ªõi'.")
    else:
        for model_name, progress in download_progress.items():
            with st.container():
                # T√≠nh to√°n ph·∫ßn trƒÉm ƒë·ªÉ hi·ªÉn th·ªã
                percentage = progress.get("pull_progress", 0) / 10 if progress.get("pull_progress") is not None else 0
                
                st.markdown(f"""
                <div class="model-card">
                    <h3>{model_name}</h3>
                    <p>Tr·∫°ng th√°i: {"Ho√†n th√†nh" if progress.get("done", False) else "ƒêang t·∫£i xu·ªëng..."}</p>
                    <div class="model-progress">
                        <div class="model-progress-inner" style="width: {percentage}%;"></div>
                    </div>
                    <p>{percentage:.1f}% ho√†n th√†nh</p>
                </div>
                """, unsafe_allow_html=True)
                
                # N√∫t h·ªßy
                if not progress.get("done", False):
                    if st.button(f"H·ªßy", key=f"cancel_{model_name}"):
                        if cancel_model_pull(model_name):
                            st.success(f"ƒê√£ h·ªßy t·∫£i xu·ªëng {model_name}")
                            time.sleep(1)
                            st.experimental_rerun()
    
    # Logic t·ª± ƒë·ªông l√†m m·ªõi
    if auto_refresh and download_progress:
        time.sleep(2)
        st.experimental_rerun()

# Footer with gradient separator
st.markdown("""
    <div style='height: 2px; background: linear-gradient(90deg, transparent, var(--primary-color), transparent);'></div>
    <div class="footer fade-in">
        <p style='margin: 0.5em 0;'>ƒê∆∞·ª£c H·ªó Tr·ª£ B·ªüi Ollama | ƒê∆∞·ª£c T·∫°o V·ªõi ‚ù§Ô∏è</p>
    </div>
""", unsafe_allow_html=True)
